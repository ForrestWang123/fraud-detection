# 实时欺诈监测系统设计文档
## 系统目标
    快速（swiftly）准确（accurately）地识别出欺诈交易，以减少损失；
## 设计挑战与应对
1. 通过MQ技术接收交易数据时，如何解决消息重复消费，消息丢失和消息积压的问题？
   - 消息重复消费：在系统网络异常，Broker主从切换，Rebalance等场景下，会发生消息重复消费。通常的方案时通过幂等操作来消除消息重复消费的影响。但对于当前的场景，分两种情况讨论：
     - 正常交易的重复消费：处理逻辑本身是幂等的，只是多了一次额外的判定，没有负面影响；
     - 欺诈交易的重复消费：如果不进行幂等操作，会存在重复记录日志的副作用。针对这种情况，我们只在Notifier中针对欺诈交易利用Redis缓存进行幂等判断(此处采用弱幂等方案)，从而节省缓存资源，降低总体处理时延；
   - 消息在消费侧丢失：如果在消费侧异步处理消息同时提前确认消息，则会导致消息丢失。为了解决这个问题，采用如下消息确认机制：
     - 正常交易：如果判定一笔交易为正常交易，直接向Broker确认消息；
     - 欺诈交易：如果判定一笔交易为欺诈交易，则不立即确认消息，待成功写入日志后再对其进行确认；
   - 消息积压：如果消费侧处理速度跟不上消息生产速度，则会导致消息堆积。采用如下措施，以避免消息积压：
     - 批量消费：利用RocketMQ 5.x新增的SimpleConsumer的批量消费功能来提高处理效率；写入日志时，采用阿里日志云Producer批量写入的方式来提高写入效率；
     - 并发消费：通过增加Pod数量和线程池的线程数来提高消费并发度
2. 如何确保系统的可演进性？  
   - 采用责任链模式来进行欺诈交易监测（detect），以便于后续添加新的监测规则；
## 系统逻辑架构图
![逻辑架构图](./LogicalArchitecture.png "逻辑架构图")
## 部署架构图
![部署架构图](./DeploymentArchitecture.png "部署架构图")
## 演进规划
当前是本服务的MVP（Minimum Viable Product），后续长期演进的特性规划如下：
1. 因时间关系，幂等和服务健康检查功能尚未实现，待开发；
2. 引入配置中心（比如apollo），可以对当前系统配置进行集中化管理，并对涉密信息进行加密;
3. 提升系统可观测性，如死信队列，采集欺诈交易数量，写入Log Service延迟，线程池Queue堆积，错误日志，消息积压等核心指标，通过Grafana Dashboard进行可视化展示；
